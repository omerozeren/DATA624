---
title: "DATA 624 - PROJECT 2"
author: "OMER OZEREN - GRACIE HAN"
output:
  html_document:
    highlight: tango
    theme: journal
    toc: yes
    toc_depth: 5
    toc_float: yes
always_allow_html: true    
---
# Project 2

This is role playing.  I am your new boss.  I am in charge of production at ABC Beverage and you are a team of data scientists reporting to me.  My leadership has told me that new regulations are requiring us to understand our manufacturing process, the predictive factors and be able to report to them our predictive model of PH.

Please use the historical data set I am providing.  Build and report the factors in BOTH a technical and non-technical report.  I like to use Word and Excel.  Please provide your non-technical report in a  business friendly readable document and your predictions in an Excel readable format.   The technical report should show clearly the models you tested and how you selected your final approach.

Please submit both Rpubs links and .rmd files or other readable formats for technical and non-technical reports.  Also submit the excel file showing the prediction of your models for pH



```{r, echo = T, results = 'hide', warning=FALSE}

library(tidyverse)
library(kableExtra)
library(xgboost)
library(plyr)
library (e1071)
library(corrplot)
library(ggplot2)
library(tidyr)
library(dplyr)
library(caret)
library(Matrix)
library(writexl)
library(psych)
```

### Load the Evaluation Data
```{r, load-Evaluationdata}
temp_file <- tempfile(fileext = ".xlsx")
download.file(url = "https://raw.githubusercontent.com/omerozeren/DATA624/master/Project2/StudentEvaluation.xlsx", 
              destfile = temp_file, 
              mode = "wb", 
              quiet = TRUE)
#load xl from temp
df_eval <- data.frame(readxl::read_excel(temp_file,skip=0))
#  Brand.Code to factor
df_eval$Brand.Code = as.factor(df_eval$Brand.Code)
```

The data set contains 267 observations and 33 variables. The variable name BrandCode is a character variable, the remaining variables are numeric. PH is the respond variable 



### Load the Train Data

```{r, load-traindata}
temp_file <- tempfile(fileext = ".xlsx")
download.file(url = "https://raw.githubusercontent.com/omerozeren/DATA624/master/Project2/StudentData.xlsx", 
              destfile = temp_file, 
              mode = "wb", 
              quiet = TRUE)
#load xl from temp
df_train <- data.frame(readxl::read_excel(temp_file,skip=0))
#  Brand.Code to factor
df_train$Brand.Code = as.factor(df_train$Brand.Code)
```



### Train Data Statistics

```{r}
dim(df_train)
```

### Train Data Number of Observations

```{r}
nrow(df_train[complete.cases(df_train),])
```

### Train Data Summary

```{r}
summary(df_train)
```

The training data set contains 2571 observations and 33 variables. The variable name BrandCode is a character variable, the remaining variables are numeric. PH is the response variable.


### Summary Statistics of Train Data

```{r message = F, warning=FALSE}
kable(describe(df_train)[,-c(1,6,7,13)], 
      caption = "Descriptive Statistics for Train Data",
      digit = 2L)
```

There are 2038 observations that has no missing values in any of the 33 columns, which indicates that the data has minimum missing values. It also means that there are 533 (2571 minus 2038) observations that has some missing values in some of their variables. 

Looking at Missing values of each of the numerical variables, the maximum NA is 212 at MFR ,followed by Filler Speed (57 missing), followed by a few variables which has missing values in 30s (PC volume, fill ounces, PSC CO2, carb pressure 1, hyd  pressure 4,), then Followed by variables which has missing values in 20s (carb pressure, PSC Phil, feel pressure, filler level).  Then, the rest of the variabl. es Have missing values that are in teens or below. 

Kurtosis For each of the variables also confirmed that MFR is highly skilled with Kurtosis=30.46.Mnf Flow have a median at 724, and mean at 704. But the range of it is from 31.4 till 868, with a wopping range of 837!

Besides MFR, the skewness of the  rest of the variables are alright. The next batch of variables with relatively high skewness (their Kurtosis value) is temperature ( endpoint one 6 ), oxygen filler ( 11.09 ), And air pressure ( 4.73 ) .

#### Visualization of Target Variable (pH)


```{r}
df_train %>%
  ggplot(aes(PH, fill = PH > 8.5)) + 
  geom_histogram(bins = 30) +
  theme_bw() +
  theme(legend.position = 'center') +
  labs(y = 'Count', title = 'PH histogram') 
```


Visualization of histogram of each individual predictor variables indicate that beside the numerical variables, there are many categorical variables (discrete variables), such as pressure.set point, aich.rel).

The obvious discrete variables are: Brand Code (ABCD 4 brands in total), Pressure Setpoint, Bowl Setpoint, PSC.CO2, Pressure Vacuume.  Each of these varaiables have no more than 10-12 unique numbers to make the count.

There are some bi-mode variables (such as pressure set point, density, hyd pressure 2, hyd pressure3).  Multi-mode (>2mode) Variables include carb flow.

Histogram also indicated the right skewness of MFR, which has a spike of counts at around the 40. 
These variables have a significant numbers of apps observations at 0: hyd pressure1,hyd pressure2,hyd pressure 3,
The close to normally distributed variables judging from the histograms are : carb pressure 1, carb  pressure 2, Carb Temp, Carb Volume, Fill Ounces, PC Volume. We chose bins=15 and facet wrap for the histograms. This findings are preserved after  changing the numbers of bins.


### Visualization of Predictors

```{r message = F}
df_train[,-c(1)]  %>%
  gather(Variable, Values) %>%
  ggplot(aes(x = Values)) +
  geom_histogram(alpha = 0.2, col = "black", bins = 15) +
  facet_wrap(~ Variable, scales = "free", nrow = 6)
```

Visualization of histogram of each individual predictor variables indicate that beside the numerical variables, there are many categorical variables (discrete variables), such as pressure.set point, aich.rel).

The obvious discrete variables are: Brand Code (ABCD 4 brands in total), Pressure Setpoint, Bowl Setpoint, PSC.CO2, Pressure Vacuume.  Each of these varaiables have no more than 10-12 unique numbers to make the count.

There are some bi-mode variables (such as pressure set point, density, hyd pressure 2, hyd pressure3).  Multi-mode (>2mode) Variables include carb flow.

Histogram also indicated the right skewness of MFR, which has a spike of counts at around the 40. 
These variables have a significant numbers of apps observations at 0: hyd pressure1,hyd pressure2,hyd pressure 3,
The close to normally distributed variables judging from the histograms are : carb pressure 1, carb  pressure 2, Carb Temp, Carb Volume, Fill Ounces, PC Volume. We chose bins=15 and facet wrap for the histograms. This findings are preserved after  changing the numbers of bins.


### Outliers Analysis with Boxplot 


```{r message = F}
df_train[,-c(1)] %>% 
  gather(Variable, Values) %>% 
  ggplot(aes( y = Values)) +
  geom_boxplot() +
  facet_wrap(~ Variable, scales = "free", nrow = 6)
```

Because some of the variables are skewed, so the box plot shows data many of these predictors are recognized as outliers.  these variables include MFR ,filler.speed, Oxygen,filler, Air.presseurer. But we predict that after transformation later on, some of these so called "outliers" will not persist. 
Besides the above mentioned four variables, these variables also have extreme outliers: PSC fill, PSC CO2, Temperature, Pressure.vacume, Alch.Rel, Carb.Rel.

Interestingly, the outcome variable pH also have a few outliers. 



### Relationships Between the Target and Explanatory Variables

This plot below indicates relationship between target and explanantory variables

```{r message = F}
df_train %>% 
  gather(-PH, -Brand.Code, key="Var", value="Value") %>% 
  ggplot(aes(x=PH, y=Value)) +
  geom_point(alpha=0.01, col = "blue") +
  facet_wrap(~ Var, scales = "free", ncol=6)
```

### Correlation


```{r corrgram, fig.width=5, fig.height=5}
corrplot::corrplot(cor(df_train[,-1], use = 'complete.obs'),
         method = 'square', type = 'lower', order = 'original',
         hclust.method = 'ward.D2', tl.cex = 0.7)
```

We find the following very strong correlations:
Carb.Volume with Density, Balling, Alch.Rel, Carb.Rel, and Balling.Lvl.
Carb.Pressure with Carb.Temp
Filler.Level with Bowl.Setpoint
Filler.Speed with MFR

Correlation plot above indicates that some explanantory variables are correleted each other. We find out explanantory variables that hig correleted each other by using findCorrelation() with using threshold as 0.6.


```{r message = F}
df_train_cor <- cor(df_train %>% select( -Brand.Code), use="complete.obs")
findCorrelation(df_train_cor, .6, names = TRUE)
```

Below shows top 10  Explanatory variables that  positively correleted highly to PH

```{r message = F}
top_df_train_cor <- df_train_cor %>% as.data.frame() %>% select(PH) %>% 
  rownames_to_column() %>% 
  arrange(desc(PH))
top_df_train_cor %>%
  top_n(10, PH)
```

Bowl Setpoint, Filler Level, Carb Flow, Pressure Vacuum are the top 5 explanatory variables that are positively associated with the PH outcome. Their correlation to the PH outcome rANGES FROM 0.36 (TOP1) TO 0.22 (TOP 5TH).  

The next set of varialbes (top 6-top10) have a correlation to outcome range from 0.196 (top 6th) to 0.098 (top 10th).
Below shows top 10  Explanatory variables that  negatively correleted highly to PH


Below shows top 10  Explanatory variables that  negatively correleted highly to PH

```{r message = F}
top_df_train_cor %>%
  top_n(-10, PH) %>%
  arrange(PH)
```

**GRACIEHAN's comments**


### Near Zero Variance Predictors

```{r}
non_zero <- nearZeroVar(df_train)
colnames(df_train[,non_zero])
```

"Hyd Pressure1" should be removed from the dataset since it hold constant values.WE are going to handle this in Model Data PreProcessing part


### Missing Values

Using VIM library to explore missing values.

```{r message = F, warning=FALSE}
library(mice)
library(VIM)
aggr(df_train, col=c('navyblue','red'), numbers=TRUE, sortVars=TRUE, labels=names(df_train), cex.axis=.7, gap=3, ylab=c("Histogram of missing data","Pattern"))
```

**GRACIEHAN's comments**


## Modeling Data PreProcessing

```{r impute}
# Train set
imputer<-mice(df_train, method = "pmm", print = FALSE, seed = 143)
df_train_imputed <-complete(imputer)
avoid_features = nearZeroVar(df_train_imputed)
df_model_train = df_train_imputed[,-avoid_features]
# Eval set
imputer<-mice(df_eval, method = "pmm", print = FALSE, seed = 143)
df_eval_imputed <-complete(imputer)
avoid_features = nearZeroVar(df_eval_imputed)
df_model_eval = df_eval_imputed[,-avoid_features]
summary(df_model_train)
```


**GRACIEHAN's comments**

let's look at the variables missing percentage after impution to check if we missing anything.

```{r}
aggr(df_model_train, col=c('navyblue','red'), numbers=TRUE, sortVars=TRUE, labels=names(df_train), cex.axis=.7, gap=3, ylab=c("Histogram of missing data","Pattern"))
```


### Splitting Data Set

Splitting dataset into training and test sets.

```{r, message=FALSE}
set.seed(123)
training.samples <- df_model_train$PH %>%
createDataPartition(p = 0.8, list = FALSE)
train_data  <- df_model_train[training.samples, ]
test_data <- df_model_train[-training.samples, ]
```

## Model Building  & Evaluation


```{r merge_results}
models_test_evaluation <- data.frame()
```

### Linear Regression  Model

```{r}
set.seed(123)
log_model <- lm(PH~.,data = train_data)
summary(log_model)
```

This is our basic banhmark model

**GRACIEHAN's comments**

### Bagged Tree Model

```{r, warning=FALSE}

trcontrol = trainControl("cv", number = 10, savePredictions=FALSE,  index = createFolds(train_data$PH, 10), verboseIter = FALSE)
set.seed(123)
bagControl = bagControl(fit = ctreeBag$fit, predict = ctreeBag$pred, aggregate = ctreeBag$aggregate)
bag_model <- train(PH ~., 
                    data = train_data, method="bag", bagControl = bagControl,
                   center = TRUE,
                   scale = TRUE,
                   trControl = trainControl("cv", number = 5),
                   tuneLength = 25)
# Make predictions
bag_pred <- predict(bag_model, newdata = test_data)
# Model performance metrics
post_rst<-postResample(obs = test_data$PH, pred=bag_pred)
models_test_evaluation <- data.frame(t(post_rst)) %>% 
    mutate(Model = "Bagged-Tree ") %>% rbind(models_test_evaluation)
```


**GRACIEHAN's comments**

### Cubist Model

```{r,warning=FALSE}
# trainControl to 10 folds cross validation
trcontrol = trainControl("cv", number = 10, savePredictions=FALSE,  index = createFolds(train_data$PH, 10), verboseIter = FALSE)
cubist_model <- train(PH ~ ., data = train_data, method = "cubist",
                        trControl = trcontrol,
                        tuneLength = 25)

# Make predictions
cubist_pred <- predict(cubist_model, newdata = test_data)
# Model performance metrics
post_rst<-postResample(obs = test_data$PH, pred=cubist_pred)
models_test_evaluation <- data.frame(t(post_rst)) %>% 
    mutate(Model = "Cubist") %>% rbind(models_test_evaluation)
```


**GRACIEHAN's comments**

### SVM Model

```{r,warning=FALSE}
# trainControl to 10 folds cross validation
trcontrol = trainControl("cv", number = 10, savePredictions=FALSE,  index = createFolds(train_data$PH, 10), verboseIter = FALSE)
svm_model <- train(PH ~.,
                data=train_data,
                method = "svmRadial",
                preProc = c("center", "scale"),
                tuneLength = 25,
                trControl = trcontrol)

# Make predictions
svm_pred <- predict(svm_model, newdata = test_data)
# Model performance metrics
post_rst<-postResample(obs = test_data$PH, pred=svm_pred)
models_test_evaluation <- data.frame(t(post_rst)) %>% 
    mutate(Model = "SVM") %>% rbind(models_test_evaluation)

```


**GRACIEHAN's comments**

### KNN Model
```{r ,warning=FALSE}
# trainControl to 10 folds cross validation
trcontrol = trainControl("cv", number = 10, savePredictions=FALSE,  index = createFolds(train_data$PH, 10), verboseIter = FALSE)
knn_model <- train(PH ~.,
                data=train_data,
                method = "knn",
                preProc = c("center", "scale"),
                tuneLength = 25,
                trControl = trcontrol)

# Make predictions
knn_pred <- predict(knn_model, newdata = test_data)
# Model performance metrics
post_rst<-postResample(obs = test_data$PH, pred=knn_pred)
models_test_evaluation <- data.frame(t(post_rst)) %>% 
    mutate(Model = "KNN ") %>% rbind(models_test_evaluation)
```


**GRACIEHAN's comments**

### Random Forest

```{r,warning=FALSE}
# trainControl to 10 folds cross validation
trcontrol = trainControl("cv", number = 10, savePredictions=FALSE,  index = createFolds(train_data$PH, 10), verboseIter = FALSE)
set.seed(123)
rf_model <- train(PH ~., 
                 data = train_data,
                 method = "rf", 
                 tuneLength = 25,
                 trControl = trcontrol
                 )

# Make predictions
rf_pred <- predict(rf_model, newdata = test_data)
# Model performance metrics
post_rst<-postResample(obs = test_data$PH, pred=rf_pred)
models_test_evaluation <- data.frame(t(post_rst)) %>% 
    mutate(Model = "Random Forest") %>% rbind(models_test_evaluation)
```


**GRACIEHAN's comments**

### Multivariate Adaptive Regression Splines (MARS)
```{r,warning=FALSE}
# trainControl to 10 folds cross validation
trcontrol = trainControl("cv", number = 10, savePredictions=FALSE,  index = createFolds(train_data$PH, 10), verboseIter = FALSE)
set.seed(123)
mars_model <- train(PH ~., 
                 data = train_data,
                 method = "earth", 
                 tuneLength = 25,
                 trControl = trcontrol
                 )

# Make predictions
mars_pred <- predict(mars_model, newdata = test_data)
# Model performance metrics
post_rst<-postResample(obs = test_data$PH, pred=mars_pred)
models_test_evaluation <- data.frame(t(post_rst)) %>% 
    mutate(Model = "MARS") %>% rbind(models_test_evaluation)
```


**GRACIEHAN's comments**


## Model Evalution Summary

```{r model_results}
models_test_evaluation %>% dplyr::select(Model, RMSE, Rsquared, MAE)
```

The table above shows our models performance.We evaluated models using below criteria:

 1. R^2 **GRACIEHAN's comments**
 2. *Root Mean Squared Error* **GRACIEHAN's comments**
 3. *Mean Absolute Error* (MAE) **GRACIEHAN's comments**
 
Based on what we've seen above table Cubist model gives best performance among the other models.So we are going to select Cubist models as champion model and predict values
by using evaluation dataset and export in excel file.

**GRACIEHAN's comments**

```{r, echo=F}
predictions <- predict(cubist_model, df_model_eval)
df_eval$PH <- round(predictions, 2) # joining predictions to original evalution data set
write.csv(df_eval, 'StudentEvaluation_PH_predictions.csv')
```
